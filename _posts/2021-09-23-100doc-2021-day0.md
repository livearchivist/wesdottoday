---
layout: post
category: 100Days
title: "Day 0: Neutron"
published: true
---

In my [opening post](/100-days-code) for 100 Days of Code, I discussed that I would work on Tapes, but while on vacation I realized how sensitive to Air Quality we really are and decided to focus on [Neutron](/neutron) instead.

Today I mostly focused on setting up the overall project, but also started to put together the Docker containers and some rough code.

I decided rather than building the entire platform and logic, it'd be best to utilize a platform that's already built for time series data analysis: InfluxDB. I've used InfluxDB in the past but never to this extent. 

To start, I'll have a primary node - Neutron - running InfluxDB and listening on `0.0.0.0`. The primary node will be bootstrapped to have a saved Dashboard using the InfluxDB API and a helper container that runs once (to be built).

The secondary node(s) - Neutrinos - where the sensors are attached, will run a custom Python script that will query sensor data, then write the data to the InfluxDB server.

There will need to be some logic built for the Neutrinos to query InfluxDB for the Raspberry Pi UUID (stored at `/boot/neutron_uuid`) to get the room assignment. 

At boot, each Neutrino will launch a helper container which will pull the `neutron_uuid` and report it to the main Neutron node on the network. Some sort of service discovery will need to be done to find the neutron.

The Neutron node will write the `neutron_uuid` to CouchDB. A small web server will run an interface that allows you to name each `neutron_uuid` and update the record in CouchDB.

The neutrino service running on each neutrino will query CouchDB on a loop until the name has been assigned. Once a name has been assigned, then the neutrino will begin recording sensor data and reporting back to InfluxDB with the `tag` updated with the Neutrino name.

I haven't pushed any code to Github yet, because I'm still deciding if I want to self host Git (which I probably will).